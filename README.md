# Supervised-Machine-Learning

The data utilized for modeling has an extensive set of personal information about each applicant. The dataset is split into two separate datasets, “application record” and “credit record.” In the “application record” dataset, there are a total of 17 variables, 12 categorical and 5 continuous. In the “credit record” dataset, there are 2 variables, 1 categorical and 1 continuous, both datasets also contain an ID key column to combine an applicant’s record with their credit data. The dataset prior to any processing has 438,557 observations. **The objective is to determine principal factors & build a classification model to predict whether a candidate should receive a credit card. ** The team utilized four supervised machine learning models: Gaussian Naïve Bayes, Logistic Regression, AdaBoost, and Random Forest. One of the challenges the team encountered when building models for this project was the imbalance between customers who were classified as risky and not risky. This imbalance made the base models weak in distinguishing the differences between those who were risky and those who are not. The team created a binary dependent variable that assigned "1" if a candidate's payment status was 60 days (about 2 months) past due, otherwise 0. 98.50% of the observations were classified as not risky, leaving only 1.50% as risky. To cure the imbalance issue in the training set, the team used an advanced oversampling technique called SMOTE. Out of the total 8 models that were built, base models plus the hyper-tuned models, the best model was the hyper-tuned random forest model. This model produced the least number of false negatives, false positives, a high accuracy score, and the best f1 score out of the set. In this paper, the team will discuss the research preparation, the methods used to cure imbalance in data, the math behind the algorithms, results of the models, and discuss how the research could be improved.
